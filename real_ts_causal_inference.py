"""
çœŸå®TabPFN-TSå’ŒDo-PFNæ¨¡å‹çš„æ—¶é—´åºåˆ—å› æœæ¨æ–­å®ç°
éœ€è¦ç¡®ä¿é¡¹ç›®ä¸­æœ‰ç›¸å…³çš„æ¨¡å‹æ–‡ä»¶å’Œä¾èµ–
"""
import warnings
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

import os
import sys
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime, timedelta
from copy import deepcopy
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('tabpfn-time-series-main')
sys.path.append('Do-PFN-main')

try:
    # TabPFN-TS å¯¼å…¥
    from tabpfn_time_series.features import (
        RunningIndexFeature, 
        CalendarFeature, 
        AutoSeasonalFeature,
        PeriodicSinCosineFeature,
        FeatureTransformer
    )
    from tabpfn_time_series.predictor import TabPFNTimeSeriesPredictor, TabPFNMode
    from autogluon.timeseries import TimeSeriesDataFrame
    
    # Do-PFN å¯¼å…¥
    from scripts.transformer_prediction_interface.base import DoPFNRegressor
    
    REAL_MODELS_AVAILABLE = True
    print("âœ… çœŸå®æ¨¡å‹ä¾èµ–åŠ è½½æˆåŠŸ")
    
except ImportError as e:
    print(f"âš ï¸ çœŸå®æ¨¡å‹ä¾èµ–åŠ è½½å¤±è´¥: {e}")
    print("å°†ä½¿ç”¨æ¨¡æ‹Ÿç‰ˆæœ¬è¿›è¡Œæ¼”ç¤º")
    REAL_MODELS_AVAILABLE = False

class RealTimeSeriesCausalInference:
    """ä½¿ç”¨çœŸå®TabPFN-TSå’ŒDo-PFNçš„æ—¶é—´åºåˆ—å› æœæ¨æ–­ç±»"""
    
    def __init__(self, use_real_models: bool = True):
        self.use_real_models = use_real_models and REAL_MODELS_AVAILABLE
        
        if self.use_real_models:
            # åˆå§‹åŒ–çœŸå®æ¨¡å‹
            self._init_real_models()
        else:
            # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹
            self._init_mock_models()
    
    def _init_real_models(self):
        """åˆå§‹åŒ–çœŸå®æ¨¡å‹"""
        try:
            # åˆå§‹åŒ–TabPFN-TSç‰¹å¾è½¬æ¢å™¨
            self.feature_transformer = FeatureTransformer([
                RunningIndexFeature(),
                CalendarFeature(components=['year', 'month', 'day']),
                AutoSeasonalFeature(),
            ])
            
            # åˆå§‹åŒ–Do-PFNå›å½’å™¨
            self.dopfn = DoPFNRegressor()
            
            print("âœ… çœŸå®æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ çœŸå®æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            print("åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å‹")
            self.use_real_models = False
            self._init_mock_models()
    
    def _init_mock_models(self):
        """åˆå§‹åŒ–æ¨¡æ‹Ÿæ¨¡å‹ï¼ˆå½“çœŸå®æ¨¡å‹ä¸å¯ç”¨æ—¶ï¼‰"""
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.preprocessing import StandardScaler
        
        self.feature_transformer = MockFeatureTransformer()
        self.dopfn = MockDoPFNWrapper()
        print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹")
    
    def generate_realistic_timeseries(self, 
                                    n_days: int = 365,
                                    intervention_probability: float = 0.2,
                                    base_intervention_effect: float = 5.0,
                                    effect_decay: float = 0.8,
                                    seasonal_components: List[int] = [7, 30, 365],
                                    noise_level: float = 1.0) -> pd.DataFrame:
        """ç”Ÿæˆæ›´çœŸå®çš„æ—¶é—´åºåˆ—æ•°æ®"""
        
        # åˆ›å»ºæ—¶é—´ç´¢å¼•
        start_date = pd.Timestamp('2023-01-01')
        dates = pd.date_range(start=start_date, periods=n_days, freq='D')
        
        # åŸºç¡€è¶‹åŠ¿ï¼ˆéçº¿æ€§ï¼‰
        t = np.arange(n_days)
        trend = 10 + 0.01 * t + 0.0001 * t**2 - 0.000001 * t**3
        
        # å¤šé‡å­£èŠ‚æ€§
        seasonal = np.zeros(n_days)
        for period in seasonal_components:
            amplitude = 5.0 / period * 100  # è°ƒæ•´æŒ¯å¹…
            seasonal += amplitude * np.sin(2 * np.pi * t / period)
        
        # ç”Ÿæˆå¹²é¢„ï¼ˆè€ƒè™‘æ—¶é—´ä¾èµ–æ€§ï¼‰
        treatment = np.zeros(n_days)
        for i in range(n_days):
            # å‘¨æœ«æ›´å®¹æ˜“æœ‰å¹²é¢„
            base_prob = intervention_probability
            if dates[i].weekday() >= 5:  # å‘¨æœ«
                base_prob *= 1.5
            
            # æœˆåˆæ›´å®¹æ˜“æœ‰å¹²é¢„
            if dates[i].day <= 5:
                base_prob *= 1.3
                
            treatment[i] = np.random.binomial(1, min(base_prob, 0.8))
        
        # åå˜é‡
        day_of_week = np.array([d.weekday() for d in dates])
        month = np.array([d.month for d in dates])
        is_weekend = (day_of_week >= 5).astype(int)
        is_month_start = np.array([d.day <= 5 for d in dates]).astype(int)
        
        # å¤æ‚çš„æ··æ‚å…³ç³»
        confounder_effect = (
            2 * is_weekend + 
            1.5 * is_month_start + 
            0.5 * np.sin(2 * np.pi * month / 12)
        )
        
        # ç›®æ ‡å˜é‡åŸºç¡€å€¼
        base_outcome = trend + seasonal + confounder_effect
        
        # æ·»åŠ æ—¶å˜çš„å¹²é¢„æ•ˆåº”
        intervention_effect = np.zeros(n_days)
        for i in range(n_days):
            # å½“å‰å’Œå†å²å¹²é¢„çš„ç´¯ç§¯æ•ˆåº”
            for lag in range(min(i + 1, 7)):  # è€ƒè™‘æœ€è¿‘7å¤©çš„æ•ˆåº”
                if i - lag >= 0 and treatment[i - lag] == 1:
                    decay_factor = effect_decay ** lag
                    seasonal_modifier = 1 + 0.3 * np.sin(2 * np.pi * (i % 365) / 365)
                    intervention_effect[i] += base_intervention_effect * decay_factor * seasonal_modifier
        
        # æœ€ç»ˆç›®æ ‡å˜é‡
        target = base_outcome + intervention_effect + np.random.normal(0, noise_level, n_days)
        
        # æ·»åŠ ä¸€äº›å¤–ç”Ÿå˜é‡
        weather_effect = 2 * np.sin(2 * np.pi * t / 365) + np.random.normal(0, 0.5, n_days)
        competitor_activity = np.random.binomial(1, 0.15, n_days)
        
        # åˆ›å»ºDataFrame
        data = pd.DataFrame({
            'timestamp': dates,
            'target': target,
            'treatment': treatment.astype(int),
            'day_of_week': day_of_week,
            'month': month,
            'is_weekend': is_weekend,
            'is_month_start': is_month_start,
            'weather_effect': weather_effect,
            'competitor_activity': competitor_activity,
            'true_intervention_effect': intervention_effect  # ç”¨äºéªŒè¯
        })
        
        return data
    
    def prepare_timeseries_for_tabpfn(self, 
                                    data: pd.DataFrame,
                                    target_col: str = 'target') -> TimeSeriesDataFrame:
        """å‡†å¤‡TabPFN-TSæ ¼å¼çš„æ•°æ®"""
        
        if not self.use_real_models:
            return data  # æ¨¡æ‹Ÿæ¨¡å¼ç›´æ¥è¿”å›
        
        # åˆ›å»ºTimeSeriesDataFrameæ ¼å¼
        df_for_ts = data[['timestamp', target_col]].copy()
        df_for_ts = df_for_ts.set_index('timestamp')
        
        # æ·»åŠ item_idç”¨äºTimeSeriesDataFrame
        df_for_ts['item_id'] = 0
        df_for_ts = df_for_ts.set_index('item_id', append=True)
        df_for_ts = df_for_ts.reorder_levels(['item_id', 'timestamp'])
        
        return TimeSeriesDataFrame(df_for_ts)
    
    def extract_tabpfn_features(self, 
                               data: pd.DataFrame,
                               target_col: str = 'target') -> pd.DataFrame:
        """ä½¿ç”¨TabPFN-TSæå–æ—¶é—´åºåˆ—ç‰¹å¾"""
        
        if not self.use_real_models:
            return self._extract_mock_features(data, target_col)
        
        try:
            # å‡†å¤‡TimeSeriesDataFrame
            ts_data = self.prepare_timeseries_for_tabpfn(data, target_col)
            
            # åº”ç”¨ç‰¹å¾è½¬æ¢
            featured_data = self.feature_transformer.transform(ts_data)
            
            # è½¬æ¢å›æ™®é€šDataFrameå¹¶æ·»åŠ å…¶ä»–å˜é‡
            result_df = featured_data.reset_index()
            
            # åˆå¹¶åŸå§‹æ•°æ®ä¸­çš„å…¶ä»–åˆ—
            merge_cols = [col for col in data.columns 
                         if col not in ['timestamp', target_col]]
            
            if merge_cols:
                original_with_idx = data.reset_index()
                for col in merge_cols:
                    if len(original_with_idx) == len(result_df):
                        result_df[col] = original_with_idx[col].values
            
            return result_df
            
        except Exception as e:
            print(f"TabPFN-TSç‰¹å¾æå–å¤±è´¥: {e}")
            print("ä½¿ç”¨å¤‡ç”¨ç‰¹å¾æå–æ–¹æ³•")
            return self._extract_mock_features(data, target_col)
    
    def _extract_mock_features(self, data: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """å¤‡ç”¨çš„ç‰¹å¾æå–æ–¹æ³•"""
        df = data.copy()
        
        # è¶‹åŠ¿ç‰¹å¾
        df['running_index'] = range(len(df))
        df['trend'] = df['running_index'] / len(df)
        
        # æ—¥å†ç‰¹å¾
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df['day_of_week_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.dayofweek / 7)
            df['day_of_week_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.dayofweek / 7)
            df['month_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.month / 12)
            df['month_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.month / 12)
            df['day_of_year_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.dayofyear / 365)
            df['day_of_year_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.dayofyear / 365)
        
        # æ»åç‰¹å¾
        for lag in [1, 3, 7, 14]:
            if lag < len(df):
                df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)
        
        # æ»‘åŠ¨çª—å£ç‰¹å¾
        for window in [3, 7, 14, 30]:
            if window < len(df):
                df[f'{target_col}_rolling_mean_{window}'] = df[target_col].rolling(window=window).mean()
                df[f'{target_col}_rolling_std_{window}'] = df[target_col].rolling(window=window).std()
                df[f'{target_col}_rolling_min_{window}'] = df[target_col].rolling(window=window).min()
                df[f'{target_col}_rolling_max_{window}'] = df[target_col].rolling(window=window).max()
        
        # å‘¨æœŸæ€§ç‰¹å¾ï¼ˆåŸºäºFFTçš„è‡ªåŠ¨å‘ç°ï¼‰
        if len(df) > 20:
            target_clean = df[target_col].fillna(method='ffill').fillna(method='bfill')
            
            # ç®€å•çš„FFTåˆ†æ
            fft_vals = np.fft.fft(target_clean - target_clean.mean())
            freqs = np.fft.fftfreq(len(target_clean))
            
            # æ‰¾åˆ°æœ€å¼ºçš„å‡ ä¸ªé¢‘ç‡åˆ†é‡
            power = np.abs(fft_vals)
            # æ’é™¤ç›´æµåˆ†é‡å’Œè´Ÿé¢‘ç‡
            valid_idx = (freqs > 0) & (freqs < 0.5)
            if np.any(valid_idx):
                valid_freqs = freqs[valid_idx]
                valid_power = power[valid_idx]
                
                # é€‰æ‹©æœ€å¼ºçš„3ä¸ªé¢‘ç‡
                top_freq_idx = np.argsort(valid_power)[-3:]
                top_freqs = valid_freqs[top_freq_idx]
                
                for i, freq in enumerate(top_freqs):
                    if freq > 0:
                        period = 1 / freq
                        df[f'auto_sin_{i}'] = np.sin(2 * np.pi * df['running_index'] / period)
                        df[f'auto_cos_{i}'] = np.cos(2 * np.pi * df['running_index'] / period)
        
        # å¡«å……ç¼ºå¤±å€¼
        df = df.fillna(0)
        
        return df
    
    def estimate_causal_effect_dopfn(self,
                                   featured_data: pd.DataFrame,
                                   intervention_col: str = 'treatment',
                                   target_col: str = 'target',
                                   query_idx: int = None,
                                   context_window: int = None) -> Dict[str, Any]:
        """ä½¿ç”¨Do-PFNä¼°è®¡å› æœæ•ˆåº”"""
        
        if query_idx is None:
            query_idx = len(featured_data) - 1
        
        if context_window is None:
            context_window = min(query_idx, 100)  # æœ€å¤šä½¿ç”¨100ä¸ªå†å²ç‚¹
        
        # ç¡®å®šä¸Šä¸‹æ–‡èŒƒå›´
        context_start = max(0, query_idx - context_window)
        context_data = featured_data.iloc[context_start:query_idx].copy()
        query_data = featured_data.iloc[[query_idx]].copy()
        
        if len(context_data) < 10:
            raise ValueError(f"ä¸Šä¸‹æ–‡æ•°æ®å¤ªå°‘: {len(context_data)} < 10")
        
        # å‡†å¤‡ç‰¹å¾
        feature_cols = [col for col in featured_data.columns 
                       if col not in [target_col, 'timestamp', 'item_id']]
        
        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—éƒ½å­˜åœ¨ä¸”æ˜¯æ•°å€¼å‹
        for col in feature_cols:
            if col not in context_data.columns:
                context_data[col] = 0
                query_data[col] = 0
            
            # è½¬æ¢ä¸ºæ•°å€¼å‹
            context_data[col] = pd.to_numeric(context_data[col], errors='coerce').fillna(0)
            query_data[col] = pd.to_numeric(query_data[col], errors='coerce').fillna(0)
        
        X_context = context_data[feature_cols].values.astype(np.float32)
        y_context = context_data[target_col].values.astype(np.float32)
        
        import torch

        # å°†numpyæ•°ç»„è½¬æ¢ä¸ºtorchå¼ é‡
        X_context_tensor = torch.from_numpy(X_context).float()
        y_context_tensor = torch.from_numpy(y_context).float()

        # æ‹Ÿåˆæ¨¡å‹
        self.dopfn.fit(X_context_tensor, y_context_tensor)

        # æ‹ŸåˆDo-PFNæ¨¡å‹
        # try:
        #     self.dopfn.fit(X_context, y_context)
        # except Exception as e:
        #     print(f"Do-PFNæ‹Ÿåˆå¤±è´¥: {e}")
        #     raise
        
        # å‡†å¤‡åäº‹å®æŸ¥è¯¢
        X_query_base = query_data[feature_cols].values.astype(np.float32)
        
        # åˆ›å»ºå¹²é¢„å’Œæ§åˆ¶åœºæ™¯
        X_query_treatment = X_query_base.copy()
        X_query_control = X_query_base.copy()

        # é¢„æµ‹æ—¶ä¹Ÿè¦è½¬æ¢
        X_query_treatment_tensor = torch.from_numpy(X_query_treatment).float()
        X_query_control_tensor = torch.from_numpy(X_query_control).float()

        pred_treatment = self.dopfn.predict_full(X_query_treatment_tensor)
        pred_control = self.dopfn.predict_full(X_query_control_tensor)
        
        # æ‰¾åˆ°treatmentåˆ—çš„ç´¢å¼•
        if intervention_col in feature_cols:
            treatment_idx = feature_cols.index(intervention_col)
            X_query_treatment[:, treatment_idx] = 1.0
            X_query_control[:, treatment_idx] = 0.0
        else:
            raise ValueError(f"å¹²é¢„åˆ— '{intervention_col}' ä¸åœ¨ç‰¹å¾åˆ—ä¸­")
        
        # é¢„æµ‹åäº‹å®ç»“æœ
        try:
            if hasattr(self.dopfn, 'predict_full'):
                pred_treatment = self.dopfn.predict_full(X_query_treatment)
                pred_control = self.dopfn.predict_full(X_query_control)
                
                # æå–é¢„æµ‹ç»“æœ
                # if isinstance(pred_treatment, dict):
                #     treatment_mean = pred_treatment.get('mean', pred_treatment.get('median', 0))[0]
                #     control_mean = pred_control.get('mean', pred_control.get('median', 0))[0]
                # æå–é¢„æµ‹ç»“æœ
                if isinstance(pred_treatment, dict):
                    treatment_mean_raw = pred_treatment.get('mean', pred_treatment.get('median', 0))
                    control_mean_raw = pred_control.get('mean', pred_control.get('median', 0))
                    
                    # å¤„ç†ä¸åŒçš„æ•°æ®ç±»å‹
                    if hasattr(treatment_mean_raw, 'cpu'):
                        treatment_mean = float(treatment_mean_raw.cpu().numpy()[0])
                        control_mean = float(control_mean_raw.cpu().numpy()[0])
                    elif isinstance(treatment_mean_raw, np.ndarray):
                        treatment_mean = float(treatment_mean_raw[0])
                        control_mean = float(control_mean_raw[0])
                    else:
                        treatment_mean = float(treatment_mean_raw)
                        control_mean = float(control_mean_raw)    
                        
                    # ä¼°è®¡ä¸ç¡®å®šæ€§
                    treatment_std = pred_treatment.get('std', np.array([0.5]))[0]
                    control_std = pred_control.get('std', np.array([0.5]))[0]
                else:
                    treatment_mean = float(pred_treatment[0])
                    control_mean = float(pred_control[0])
                    treatment_std = 0.5
                    control_std = 0.5
            else:
                # å¤‡ç”¨é¢„æµ‹æ–¹æ³•
                treatment_mean = float(self.dopfn.predict(X_query_treatment)[0])
                control_mean = float(self.dopfn.predict(X_query_control)[0])
                treatment_std = 0.5
                control_std = 0.5
                
        except Exception as e:
            print(f"Do-PFNé¢„æµ‹å¤±è´¥: {e}")
            raise
        
        # è®¡ç®—å› æœæ•ˆåº”
        causal_effect = {
            'ate': treatment_mean - control_mean,
            'treatment_outcome': treatment_mean,
            'control_outcome': control_mean,
            'uncertainty': np.sqrt(treatment_std**2 + control_std**2),
            'query_idx': query_idx,
            'context_length': len(context_data)
        }
        
        return causal_effect
    
    def run_time_varying_causal_analysis(self,
                                       data: pd.DataFrame,
                                       intervention_col: str = 'treatment',
                                       target_col: str = 'target',
                                       start_idx: int = None,
                                       end_idx: int = None,
                                       step_size: int = 1,
                                       min_context: int = 30) -> pd.DataFrame:
        """è¿è¡Œæ—¶å˜å› æœåˆ†æ"""
        
        print("å¼€å§‹æ—¶å˜å› æœåˆ†æ...")
        
        # 1. ç‰¹å¾å·¥ç¨‹
        print("1. æ‰§è¡Œç‰¹å¾å·¥ç¨‹...")
        featured_data = self.extract_tabpfn_features(data, target_col)
        print(f"   ç‰¹å¾ç»´åº¦: {featured_data.shape}")
        
        # 2. è®¾ç½®åˆ†æèŒƒå›´
        if start_idx is None:
            start_idx = min_context
        if end_idx is None:
            end_idx = len(data) - 1
        
        query_indices = range(start_idx, end_idx + 1, step_size)
        print(f"   åˆ†æèŒƒå›´: {start_idx} åˆ° {end_idx}, æ­¥é•¿: {step_size}")
        print(f"   æ€»æŸ¥è¯¢ç‚¹: {len(query_indices)}")
        
        # 3. é€ç‚¹ä¼°è®¡å› æœæ•ˆåº”
        results = []
        successful_estimates = 0
        
        for i, query_idx in enumerate(query_indices):
            try:
                print(f"   å¤„ç†æŸ¥è¯¢ç‚¹ {query_idx} ({i+1}/{len(query_indices)})", end='')
                
                effect = self.estimate_causal_effect_dopfn(
                    featured_data,
                    intervention_col=intervention_col,
                    target_col=target_col,
                    query_idx=query_idx
                )
                
                # æ·»åŠ åŸå§‹æ•°æ®ä¿¡æ¯
                original_row = data.iloc[query_idx]
                result_row = {
                    'query_idx': query_idx,
                    'timestamp': original_row.get('timestamp', query_idx),
                    'ate': effect['ate'],
                    'treatment_outcome': effect['treatment_outcome'],
                    'control_outcome': effect['control_outcome'],
                    'uncertainty': effect['uncertainty'],
                    'context_length': effect['context_length'],
                    'actual_treatment': original_row[intervention_col],
                    'actual_outcome': original_row[target_col],
                }
                
                # å¦‚æœæœ‰çœŸå®æ•ˆåº”ï¼Œæ·»åŠ è¿›æ¥
                if 'true_intervention_effect' in original_row:
                    result_row['true_effect'] = original_row['true_intervention_effect']
                
                results.append(result_row)
                successful_estimates += 1
                print(" âœ“")
                
            except Exception as e:
                print(f" âœ— (é”™è¯¯: {str(e)[:50]})")
                continue
        
        print(f"\n   æˆåŠŸä¼°è®¡: {successful_estimates}/{len(query_indices)} ä¸ªæ—¶é—´ç‚¹")
        
        if not results:
            raise ValueError("æ²¡æœ‰æˆåŠŸçš„å› æœæ•ˆåº”ä¼°è®¡")
        
        return pd.DataFrame(results)
    
    def plot_comprehensive_analysis(self,
                                  original_data: pd.DataFrame,
                                  causal_results: pd.DataFrame,
                                  intervention_col: str = 'treatment',
                                  target_col: str = 'target'):
        """ç»˜åˆ¶å…¨é¢çš„å› æœåˆ†æç»“æœ"""
        
        fig, axes = plt.subplots(4, 1, figsize=(16, 16))
        
        # 1. åŸå§‹æ—¶é—´åºåˆ—å’Œå¹²é¢„
        ax1 = axes[0]
        ax1.plot(original_data['timestamp'], original_data[target_col], 
                label='ç›®æ ‡å˜é‡', color='blue', alpha=0.7, linewidth=1)
        
        # æ ‡è®°å¹²é¢„ç‚¹
        intervention_points = original_data[original_data[intervention_col] == 1]
        if len(intervention_points) > 0:
            ax1.scatter(intervention_points['timestamp'], intervention_points[target_col],
                       color='red', s=30, alpha=0.8, label='å¹²é¢„ç‚¹', zorder=5)
        
        ax1.set_title('åŸå§‹æ—¶é—´åºåˆ—æ•°æ®', fontsize=14, fontweight='bold')
        ax1.set_ylabel('ç›®æ ‡å˜é‡å€¼')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. ä¼°è®¡çš„å› æœæ•ˆåº”
        ax2 = axes[1]
        ax2.plot(causal_results['timestamp'], causal_results['ate'], 
                color='green', linewidth=2, label='ä¼°è®¡çš„ATE')
        
        # ä¸ç¡®å®šæ€§åŒºé—´
        ax2.fill_between(causal_results['timestamp'],
                        causal_results['ate'] - causal_results['uncertainty'],
                        causal_results['ate'] + causal_results['uncertainty'],
                        alpha=0.3, color='green', label='95%ç½®ä¿¡åŒºé—´')
        
        # å¦‚æœæœ‰çœŸå®æ•ˆåº”ï¼Œæ˜¾ç¤ºå¯¹æ¯”
        if 'true_effect' in causal_results.columns:
            # è®¡ç®—æ¯ä¸ªæ—¶é—´ç‚¹çš„çœŸå®ç¬æ—¶æ•ˆåº”å˜åŒ–
            true_effects = []
            for idx in causal_results['query_idx']:
                if idx > 0:
                    current_true = original_data.iloc[idx]['true_intervention_effect']
                    prev_true = original_data.iloc[idx-1]['true_intervention_effect']
                    true_effects.append(current_true - prev_true)
                else:
                    true_effects.append(original_data.iloc[idx]['true_intervention_effect'])
            
            ax2.plot(causal_results['timestamp'], true_effects,
                    color='orange', linewidth=2, linestyle='--', label='çœŸå®æ•ˆåº”', alpha=0.8)
        
        ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        ax2.set_title('å› æœæ•ˆåº”ä¼°è®¡ vs çœŸå®æ•ˆåº”', fontsize=14, fontweight='bold')
        ax2.set_ylabel('å› æœæ•ˆåº”')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. åäº‹å®é¢„æµ‹å¯¹æ¯”
        ax3 = axes[2]
        ax3.plot(causal_results['timestamp'], causal_results['treatment_outcome'],
                color='red', label='å¹²é¢„æƒ…æ™¯é¢„æµ‹', linewidth=2)
        ax3.plot(causal_results['timestamp'], causal_results['control_outcome'],
                color='blue', label='æ§åˆ¶æƒ…æ™¯é¢„æµ‹', linewidth=2)
        ax3.plot(causal_results['timestamp'], causal_results['actual_outcome'],
                color='black', alpha=0.7, linestyle=':', linewidth=2, label='å®é™…è§‚æµ‹å€¼')
        
        ax3.set_title('åäº‹å®é¢„æµ‹å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax3.set_ylabel('é¢„æµ‹ç»“æœ')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. è¯¯å·®åˆ†æ
        ax4 = axes[3]
        if 'true_effect' in causal_results.columns:
            true_effects_instant = []
            for idx in causal_results['query_idx']:
                if idx > 0:
                    current_true = original_data.iloc[idx]['true_intervention_effect']
                    prev_true = original_data.iloc[idx-1]['true_intervention_effect']
                    true_effects_instant.append(current_true - prev_true)
                else:
                    true_effects_instant.append(original_data.iloc[idx]['true_intervention_effect'])
            
            estimation_error = causal_results['ate'] - true_effects_instant
            ax4.plot(causal_results['timestamp'], estimation_error,
                    color='purple', linewidth=2, label='ä¼°è®¡è¯¯å·®')
            ax4.fill_between(causal_results['timestamp'],
                            -causal_results['uncertainty'],
                            causal_results['uncertainty'],
                            alpha=0.3, color='gray', label='ä¸ç¡®å®šæ€§åŒºé—´')
            
            rmse = np.sqrt(np.mean(estimation_error**2))
            mae = np.mean(np.abs(estimation_error))
            ax4.set_title(f'ä¼°è®¡è¯¯å·®åˆ†æ (RMSE: {rmse:.3f}, MAE: {mae:.3f})', 
                         fontsize=14, fontweight='bold')
        else:
            # å¦‚æœæ²¡æœ‰çœŸå®æ•ˆåº”ï¼Œæ˜¾ç¤ºé¢„æµ‹æ®‹å·®
            prediction_residuals = causal_results['actual_outcome'] - causal_results['treatment_outcome']
            ax4.plot(causal_results['timestamp'], prediction_residuals,
                    color='purple', linewidth=2, label='é¢„æµ‹æ®‹å·®')
            ax4.set_title('é¢„æµ‹æ®‹å·®åˆ†æ', fontsize=14, fontweight='bold')
        
        ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        ax4.set_xlabel('æ—¶é—´')
        ax4.set_ylabel('è¯¯å·®')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        # plt.show()
        plt.savefig('causal_analysis_report.png', dpi=300)
        
        # æ‰“å°ç»Ÿè®¡æ±‡æ€»
        self._print_analysis_summary(causal_results, original_data)
    
    def _print_analysis_summary(self, causal_results: pd.DataFrame, original_data: pd.DataFrame):
        """æ‰“å°åˆ†ææ±‡æ€»ç»Ÿè®¡"""
        print("\n" + "="*60)
        print("ğŸ“Š æ—¶é—´åºåˆ—å› æœåˆ†ææ±‡æ€»æŠ¥å‘Š")
        print("="*60)
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"ğŸ“ˆ åˆ†ææ—¶é—´èŒƒå›´: {len(causal_results)} ä¸ªæ—¶é—´ç‚¹")
        print(f"ğŸ“ˆ å¹³å‡ä¼°è®¡å› æœæ•ˆåº”: {causal_results['ate'].mean():.4f}")
        print(f"ğŸ“ˆ å› æœæ•ˆåº”æ ‡å‡†å·®: {causal_results['ate'].std():.4f}")
        print(f"ğŸ“ˆ å¹³å‡ä¸ç¡®å®šæ€§: {causal_results['uncertainty'].mean():.4f}")
        
        # æ˜¾è‘—æ€§åˆ†æ
        significant_positive = (causal_results['ate'] > causal_results['uncertainty']).sum()
        significant_negative = (causal_results['ate'] < -causal_results['uncertainty']).sum()
        
        print(f"\nğŸ” æ˜¾è‘—æ€§åˆ†æ:")
        print(f"   æ˜¾è‘—æ­£æ•ˆåº”æ—¶é—´ç‚¹: {significant_positive} ({significant_positive/len(causal_results)*100:.1f}%)")
        print(f"   æ˜¾è‘—è´Ÿæ•ˆåº”æ—¶é—´ç‚¹: {significant_negative} ({significant_negative/len(causal_results)*100:.1f}%)")
        print(f"   ä¸æ˜¾è‘—æ—¶é—´ç‚¹: {len(causal_results) - significant_positive - significant_negative}")
        
        # çœŸå®æ•ˆåº”å¯¹æ¯”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if 'true_effect' in causal_results.columns:
            true_effects = []
            for idx in causal_results['query_idx']:
                if idx > 0:
                    current_true = original_data.iloc[idx]['true_intervention_effect']
                    prev_true = original_data.iloc[idx-1]['true_intervention_effect']
                    true_effects.append(current_true - prev_true)
                else:
                    true_effects.append(original_data.iloc[idx]['true_intervention_effect'])
            
            estimation_error = causal_results['ate'] - true_effects
            rmse = np.sqrt(np.mean(estimation_error**2))
            mae = np.mean(np.abs(estimation_error))
            correlation = np.corrcoef(causal_results['ate'], true_effects)[0, 1]
            
            print(f"\nâœ… ä¼°è®¡ç²¾åº¦è¯„ä¼°:")
            print(f"   å‡æ–¹æ ¹è¯¯å·®(RMSE): {rmse:.4f}")
            print(f"   å¹³å‡ç»å¯¹è¯¯å·®(MAE): {mae:.4f}")
            print(f"   ç›¸å…³ç³»æ•°: {correlation:.4f}")
        
        # å¹²é¢„åˆ†æ
        total_interventions = original_data[original_data['treatment'] == 1].shape[0]
        intervention_rate = total_interventions / len(original_data)
        
        print(f"\nğŸ’Š å¹²é¢„ç»Ÿè®¡:")
        print(f"   æ€»å¹²é¢„æ¬¡æ•°: {total_interventions}")
        print(f"   å¹²é¢„ç‡: {intervention_rate:.3f}")
        
        # æ¨¡å‹æ€§èƒ½
        avg_context_length = causal_results['context_length'].mean()
        print(f"\nğŸ”§ æ¨¡å‹è®¾ç½®:")
        print(f"   å¹³å‡ä¸Šä¸‹æ–‡é•¿åº¦: {avg_context_length:.1f}")
        print(f"   ä½¿ç”¨çœŸå®æ¨¡å‹: {'æ˜¯' if self.use_real_models else 'å¦'}")


# æ¨¡æ‹Ÿç±»å®šä¹‰ï¼ˆå½“çœŸå®æ¨¡å‹ä¸å¯ç”¨æ—¶ï¼‰
class MockFeatureTransformer:
    def transform(self, data):
        return data

class MockDoPFNWrapper:
    def __init__(self):
        from sklearn.ensemble import RandomForestRegressor
        self.model = RandomForestRegressor(n_estimators=50, random_state=42)
        self.is_fitted = False
    
    def fit(self, X, y):
        self.model.fit(X, y)
        self.is_fitted = True
    
    def predict_full(self, X):
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹æœªæ‹Ÿåˆ")
        
        pred = self.model.predict(X)
        std_est = np.abs(pred) * 0.1 + 0.5  # ç®€å•çš„ä¸ç¡®å®šæ€§ä¼°è®¡
        
        return {
            'mean': pred,
            'std': std_est,
            'median': pred
        }
    
    def predict(self, X):
        return self.model.predict(X)


def main_comprehensive_demo():
    """å®Œæ•´çš„æ—¶é—´åºåˆ—å› æœæ¨æ–­æ¼”ç¤º"""
    
    print("ğŸš€ å¯åŠ¨æ—¶é—´åºåˆ—å› æœæ¨æ–­ç³»ç»Ÿ")
    print("="*60)
    
    # 1. åˆå§‹åŒ–ç³»ç»Ÿ
    causal_system = RealTimeSeriesCausalInference(use_real_models=True)
    
    # 2. ç”Ÿæˆæ›´çœŸå®çš„æ•°æ®
    print("\nğŸ“Š ç”ŸæˆçœŸå®æ„Ÿæ—¶é—´åºåˆ—æ•°æ®...")
    data = causal_system.generate_realistic_timeseries(
        n_days=200,
        intervention_probability=0.25,
        base_intervention_effect=4.0,
        effect_decay=0.7,
        seasonal_components=[7, 30, 90],
        noise_level=1.2
    )
    
    print(f"   æ•°æ®ç»´åº¦: {data.shape}")
    print(f"   æ—¶é—´èŒƒå›´: {data['timestamp'].min()} åˆ° {data['timestamp'].max()}")
    print(f"   å¹²é¢„æ€»æ•°: {data['treatment'].sum()}")
    print(f"   ç›®æ ‡å˜é‡èŒƒå›´: {data['target'].min():.2f} åˆ° {data['target'].max():.2f}")
    
    # 3. è¿è¡Œæ—¶å˜å› æœåˆ†æ
    print("\nğŸ” æ‰§è¡Œæ—¶å˜å› æœåˆ†æ...")
    causal_results = causal_system.run_time_varying_causal_analysis(
        data,
        intervention_col='treatment',
        target_col='target',
        start_idx=50,  # ä»ç¬¬50å¤©å¼€å§‹åˆ†æ
        end_idx=180,   # åˆ°ç¬¬180å¤©ç»“æŸ
        step_size=2,   # æ¯2å¤©åˆ†æä¸€æ¬¡
        min_context=40  # æœ€å°‘40å¤©çš„å†å²æ•°æ®
    )
    
    # 4. å¯è§†åŒ–å’Œåˆ†æ
    print("\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
    causal_system.plot_comprehensive_analysis(
        data, causal_results,
        intervention_col='treatment',
        target_col='target'
    )
    
    return {
        'system': causal_system,
        'data': data,
        'results': causal_results
    }

if __name__ == "__main__":
    results = main_comprehensive_demo()